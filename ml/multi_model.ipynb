{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from feature_engine.selection import DropFeatures\n",
    "\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELE JP BOX OFFICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation du pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4546 entries, 0 to 4546\n",
      "Data columns (total 20 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   titre           4546 non-null   object  \n",
      " 1   acteurs         4546 non-null   object  \n",
      " 2   budget          4546 non-null   int64   \n",
      " 3   compositeur     4546 non-null   object  \n",
      " 4   semaine_fr      4546 non-null   object  \n",
      " 5   semaine_usa     4546 non-null   object  \n",
      " 6   duree           4546 non-null   int64   \n",
      " 7   entrees_fr      4546 non-null   int64   \n",
      " 8   franchise       4546 non-null   category\n",
      " 9   genres          4546 non-null   category\n",
      " 10  pegi_fr         4546 non-null   category\n",
      " 11  pegi_usa        4546 non-null   category\n",
      " 12  producteur      4546 non-null   object  \n",
      " 13  realisateur     4546 non-null   object  \n",
      " 14  entrees_usa     4546 non-null   int64   \n",
      " 15  salles_fr       4546 non-null   int64   \n",
      " 16  studio          4546 non-null   object  \n",
      " 17  is_compositeur  4546 non-null   category\n",
      " 18  annee           4546 non-null   int64   \n",
      " 19  origine         4546 non-null   category\n",
      "dtypes: category(6), int64(6), object(8)\n",
      "memory usage: 559.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle(\"datasets/dataset_sql_cleaned.pkl\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netoyage db post netoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4546 entries, 0 to 4546\n",
      "Data columns (total 19 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   acteurs         4546 non-null   object  \n",
      " 1   budget          4546 non-null   int64   \n",
      " 2   compositeur     4546 non-null   object  \n",
      " 3   semaine_fr      4546 non-null   object  \n",
      " 4   semaine_usa     4546 non-null   object  \n",
      " 5   duree           4546 non-null   int64   \n",
      " 6   entrees_fr      4546 non-null   int64   \n",
      " 7   franchise       4546 non-null   category\n",
      " 8   genres          4546 non-null   category\n",
      " 9   pegi_fr         4546 non-null   category\n",
      " 10  pegi_usa        4546 non-null   category\n",
      " 11  producteur      4546 non-null   object  \n",
      " 12  realisateur     4546 non-null   object  \n",
      " 13  entrees_usa     4546 non-null   int64   \n",
      " 14  salles_fr       4546 non-null   int64   \n",
      " 15  studio          4546 non-null   object  \n",
      " 16  is_compositeur  4546 non-null   category\n",
      " 17  annee           4546 non-null   int64   \n",
      " 18  origine         4546 non-null   category\n",
      "dtypes: category(6), int64(6), object(7)\n",
      "memory usage: 524.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acteurs</th>\n",
       "      <th>budget</th>\n",
       "      <th>compositeur</th>\n",
       "      <th>semaine_fr</th>\n",
       "      <th>semaine_usa</th>\n",
       "      <th>duree</th>\n",
       "      <th>entrees_fr</th>\n",
       "      <th>franchise</th>\n",
       "      <th>genres</th>\n",
       "      <th>pegi_fr</th>\n",
       "      <th>pegi_usa</th>\n",
       "      <th>producteur</th>\n",
       "      <th>realisateur</th>\n",
       "      <th>entrees_usa</th>\n",
       "      <th>salles_fr</th>\n",
       "      <th>studio</th>\n",
       "      <th>is_compositeur</th>\n",
       "      <th>annee</th>\n",
       "      <th>origine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vincent cassel, gérard lanvin, simon pegg,  qu...</td>\n",
       "      <td>90000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>2009-07-03</td>\n",
       "      <td>2009-07-01</td>\n",
       "      <td>87</td>\n",
       "      <td>2403734</td>\n",
       "      <td>1</td>\n",
       "      <td>animation</td>\n",
       "      <td>Tous publics</td>\n",
       "      <td>PG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>41690382</td>\n",
       "      <td>783</td>\n",
       "      <td>20th century fox</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>etats-unis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gerard butler, jodie foster</td>\n",
       "      <td>37000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>2008-04-09</td>\n",
       "      <td>2008-04-04</td>\n",
       "      <td>95</td>\n",
       "      <td>243392</td>\n",
       "      <td>0</td>\n",
       "      <td>film familial</td>\n",
       "      <td>Tous publics</td>\n",
       "      <td>PG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>13210579</td>\n",
       "      <td>399</td>\n",
       "      <td>snd</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>etats-unis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robert pattinson, kristen stewart</td>\n",
       "      <td>37000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>2009-01-07</td>\n",
       "      <td>2008-11-21</td>\n",
       "      <td>120</td>\n",
       "      <td>755835</td>\n",
       "      <td>1</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>Tous publics</td>\n",
       "      <td>PG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>69637740</td>\n",
       "      <td>455</td>\n",
       "      <td>snd</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>etats-unis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alain chabat</td>\n",
       "      <td>11378260</td>\n",
       "      <td>-1</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>-1</td>\n",
       "      <td>98</td>\n",
       "      <td>112363</td>\n",
       "      <td>0</td>\n",
       "      <td>comédie</td>\n",
       "      <td>Tous publics</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>eric lartigau</td>\n",
       "      <td>-1</td>\n",
       "      <td>387</td>\n",
       "      <td>gaumont</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>france</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scarlett johansson, demi moore</td>\n",
       "      <td>20000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>2017-06-16</td>\n",
       "      <td>101</td>\n",
       "      <td>66666</td>\n",
       "      <td>0</td>\n",
       "      <td>comédie</td>\n",
       "      <td>Tous publics</td>\n",
       "      <td>R</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>8004283</td>\n",
       "      <td>145</td>\n",
       "      <td>sony pictures</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>etats-unis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             acteurs    budget compositeur  \\\n",
       "0  vincent cassel, gérard lanvin, simon pegg,  qu...  90000000          -1   \n",
       "1                        gerard butler, jodie foster  37000000          -1   \n",
       "2                  robert pattinson, kristen stewart  37000000          -1   \n",
       "3                                       alain chabat  11378260          -1   \n",
       "4                     scarlett johansson, demi moore  20000000          -1   \n",
       "\n",
       "   semaine_fr semaine_usa  duree  entrees_fr franchise         genres  \\\n",
       "0  2009-07-03  2009-07-01     87     2403734         1      animation   \n",
       "1  2008-04-09  2008-04-04     95      243392         0  film familial   \n",
       "2  2009-01-07  2008-11-21    120      755835         1        fantasy   \n",
       "3  2020-02-05          -1     98      112363         0        comédie   \n",
       "4  2017-08-02  2017-06-16    101       66666         0        comédie   \n",
       "\n",
       "          pegi_fr pegi_usa producteur    realisateur  entrees_usa  salles_fr  \\\n",
       "0   Tous publics        PG         -1             -1     41690382        783   \n",
       "1   Tous publics        PG         -1             -1     13210579        399   \n",
       "2   Tous publics        PG         -1             -1     69637740        455   \n",
       "3   Tous publics        -1         -1  eric lartigau           -1        387   \n",
       "4   Tous publics         R         -1             -1      8004283        145   \n",
       "\n",
       "             studio is_compositeur  annee     origine  \n",
       "0  20th century fox              0   2009  etats-unis  \n",
       "1               snd              0   2008  etats-unis  \n",
       "2               snd              0   2009  etats-unis  \n",
       "3           gaumont              0   2020      france  \n",
       "4     sony pictures              0   2017  etats-unis  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols_drop = [\"titre\"]\n",
    "\n",
    "data = data.drop(cols_drop, axis=1)\n",
    "\n",
    "display(data.info())\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_test = pipe_cb.predict(X_test)\n",
    "\n",
    "# score = pipe_cb.score(X_test, y_test)\n",
    "# r2 = r2_score(y_test, pred_test)\n",
    "# mse = mean_squared_error(y_test, pred_test)\n",
    "# rmse = np.sqrt(mse)\n",
    "# mae = mean_absolute_error(y_test, pred_test)\n",
    "\n",
    "# print(\"Score :\", score)\n",
    "# print(\"Score R2 :\", r2)\n",
    "# print(\"Score MSE :\", mse)\n",
    "# print(\"Score RMSE\", rmse)\n",
    "# print(\"Score MAE :\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_features = pipe_cb.named_steps['columntransformer'].get_feature_names_out()\n",
    "# cb_model = pipe_cb.named_steps['catboostregressor']\n",
    "\n",
    "# feat_imp = pd.DataFrame({'features': preprocessed_features, 'score': cb_model.feature_importances_})\n",
    "# feat_imp.sort_values(by=['score'], ascending=False, inplace=True)\n",
    "# display(feat_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline modèle Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data.drop(\"entrees_fr\", axis=1)\n",
    "# y = data.entrees_premiere_semaine\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=42)\n",
    "\n",
    "# object_cols = list(X.select_dtypes(include=[\"object\"]).columns)\n",
    "# display(object_cols)\n",
    "# cat_cols = list(X.select_dtypes(include=[\"category\"]).columns.drop([\"annee\"]))\n",
    "# num_cols = list(X.select_dtypes(include=[\"int64\"]).columns)\n",
    "# year_col = [\"annee\"]\n",
    "\n",
    "# # Ordinal encoding for ApprovalFY\n",
    "# unique_years = sorted(data[\"annee\"].unique())\n",
    "\n",
    "# preprocessing = ColumnTransformer([\n",
    "#         (\"onehot\", OneHotEncoder(), cat_cols),\n",
    "#         # (\"frequency\", CountFrequencyEncoder(encoding_method=\"frequency\", missing_values=\"ignore\"), object_cols),\n",
    "#         (\"scaler\", StandardScaler(), num_cols),\n",
    "#         (\"ordinal\", OrdinalEncoder(categories=[unique_years], handle_unknown=\"use_encoded_value\", unknown_value=2000), year_col),\n",
    "#     ],\n",
    "#     remainder=\"passthrough\",\n",
    "#     verbose_feature_names_out=False,\n",
    "# )\n",
    "\n",
    "# pre_fit = preprocessing.fit(X_train)\n",
    "# fit_cols = pre_fit.get_feature_names_out()\n",
    "\n",
    "# cat_indices = []\n",
    "# for i, col_name in enumerate(fit_cols):\n",
    "#     if col_name in cat_cols:\n",
    "#         cat_indices.append(i)\n",
    "\n",
    "\n",
    "\n",
    "# lasso_model = Lasso(alpha=1, random_state=42)\n",
    "\n",
    "# pipe_lasso = make_pipeline(preprocessing, lasso_model)\n",
    "\n",
    "# pipe_lasso.fit(X_train, y_train)\n",
    "\n",
    "# # pipe_lasso.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline modèle Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lr_model = LinearRegression()\n",
    "\n",
    "# pipe_lr = make_pipeline(preprocessing, lr_model)\n",
    "\n",
    "# display(pipe_lr)\n",
    "\n",
    "# pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "# # test_score = pipe_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline modèle XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs hyperparamètres trouvés :\n",
      "{'xgbregressor__colsample_bytree': 0.9, 'xgbregressor__gamma': 0, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 4, 'xgbregressor__min_child_weight': 0.5, 'xgbregressor__n_estimators': 100, 'xgbregressor__reg_alpha': 0.05, 'xgbregressor__reg_lambda': 0.01, 'xgbregressor__scale_pos_weight': 0, 'xgbregressor__subsample': 1.0}\n",
      "Score : -40514477251.11582\n",
      "Score R2 : 0.7846639711988117\n",
      "Score MSE : 40514477251.11582\n",
      "Score RMSE 201282.08378073748\n",
      "Score MAE : 105518.0915219485\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from category_encoders.count import CountEncoder\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "X = data.drop(\"entrees_fr\", axis=1)\n",
    "y = data.entrees_fr\n",
    "\n",
    "# cols_drop = [\"acteurs\", \"compositeur\", \"date\", \"pays\", \"producteur\", \"realisateur\", \"titre\"]\n",
    "# Liste des colonnes à garder\n",
    "cols_keep = [\"budget\", \"duree\", \"franchise\", \"genres\", \"pegi_fr\", \"pegi_usa\", \"entrees_usa\", \"salles_fr\", \"studio\", \"is_compositeur\", \"annee\", \"origine\"]\n",
    "X = X[cols_keep]\n",
    "\n",
    "X[\"franchise\"] = X[\"franchise\"].astype(str)\n",
    "X[\"genres\"] = X[\"genres\"].astype(str)\n",
    "X[\"pegi_fr\"] = X[\"pegi_fr\"].astype(str)\n",
    "X[\"pegi_usa\"] = X[\"pegi_usa\"].astype(str)\n",
    "X[\"is_compositeur\"] = X[\"is_compositeur\"].astype(str)\n",
    "# X[\"annee\"] = X[\"annee\"].astype(str)\n",
    "X[\"origine\"] = X[\"origine\"].astype(str)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=42)\n",
    "\n",
    "object_cols = list(X.select_dtypes(include=[\"object\"]).columns)\n",
    "cat_cols = list(X.select_dtypes(include=[\"category\"]).columns)#.drop([\"annee\"]))\n",
    "num_cols = list(X.select_dtypes(include=[\"int64\"]).columns.drop([\"annee\"]))\n",
    "year_col = [\"annee\"]\n",
    "\n",
    "# Ordinal encoding for ApprovalFY\n",
    "unique_years = sorted(data[\"annee\"].unique())\n",
    "preprocessing = ColumnTransformer([\n",
    "        (\"onehot\", OneHotEncoder(), cat_cols),\n",
    "        (\"frequency\", CountEncoder(), object_cols),\n",
    "        (\"scaler\", StandardScaler(), num_cols),\n",
    "        (\"ordinal\", OrdinalEncoder(categories=[unique_years], handle_unknown=\"use_encoded_value\", unknown_value=2000), year_col),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Définition de la grille des hyperparamètres à tester\n",
    "# {'xgbregressor__colsample_bytree': 0.9, 'xgbregressor__gamma': 0, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 4, \n",
    "#'xgbregressor__min_child_weight': 0.5, 'xgbregressor__n_estimators': 100, 'xgbregressor__reg_alpha': 0.05, 'xgbregressor__reg_lambda': 0.01,\n",
    "# 'xgbregressor__scale_pos_weight': 1, 'xgbregressor__subsample': 1.0}\n",
    "\n",
    "param_grid = {\n",
    "    'xgbregressor__n_estimators': [100],#[50, 100, 150],\n",
    "    'xgbregressor__learning_rate': [0.1],#[0.5, 0.1, 1.5],\n",
    "    'xgbregressor__max_depth': [4],#[3.4, 4, 4.5],\n",
    "    'xgbregressor__min_child_weight': [0.5],#[0.5, 1, 2],\n",
    "    'xgbregressor__gamma': [0],#[0.0, 0.01, 0.02],\n",
    "    'xgbregressor__subsample': [1.0],#[0.8, 0.9, 1.0],\n",
    "    'xgbregressor__colsample_bytree': [0.9],#[0.8, 0.9, 1.0],\n",
    "    'xgbregressor__reg_alpha': [0.05],#[0, 0.001, 0.005, 0.01, 0.05],\n",
    "    'xgbregressor__reg_lambda': [0.01],#[0, 0.001, 0.005, 0.01, 0.05],\n",
    "    'xgbregressor__scale_pos_weight': [0],#[1, 2, 3, 4, 5],\n",
    "    # Ajoutez d'autres hyperparamètres à tester selon vos besoins\n",
    "}\n",
    "\n",
    "# Création du pipeline\n",
    "pipe_xgb = make_pipeline(preprocessing, xgb_model)\n",
    "\n",
    "# Recherche par grille des meilleurs hyperparamètres\n",
    "grid_search = GridSearchCV(pipe_xgb, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Affichage des meilleurs hyperparamètres\n",
    "print(\"Meilleurs hyperparamètres trouvés :\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Prédiction sur l'ensemble de test avec les meilleurs paramètres\n",
    "pred_test = grid_search.predict(X_test)\n",
    "\n",
    "# Calcul des métriques d'évaluation\n",
    "score = grid_search.score(X_test, y_test)\n",
    "r2 = r2_score(y_test, pred_test)\n",
    "mse = mean_squared_error(y_test, pred_test)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, pred_test)\n",
    "\n",
    "print(\"Score :\", score)\n",
    "print(\"Score R2 :\", r2)\n",
    "print(\"Score MSE :\", mse)\n",
    "print(\"Score RMSE\", rmse)\n",
    "print(\"Score MAE :\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extraction des noms des fonctionnalités prétraitées\n",
    "# preprocessed_features = grid_search.best_estimator_.named_steps['columntransformer'].get_feature_names_out()\n",
    "\n",
    "# # Extraction du modèle XGBoost du meilleur estimateur\n",
    "# xgb_model = grid_search.best_estimator_.named_steps['xgbregressor']\n",
    "\n",
    "# # Création d'un DataFrame pour stocker l'importance des fonctionnalités\n",
    "# feat_imp = pd.DataFrame({'features': preprocessed_features, 'score': xgb_model.feature_importances_})\n",
    "\n",
    "# # Tri des fonctionnalités par importance\n",
    "# feat_imp.sort_values(by=['score'], ascending=False, inplace=True)\n",
    "\n",
    "# # Affichage du DataFrame contenant l'importance des fonctionnalités\n",
    "# display(feat_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline modele Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/feature_engine/encoding/base_encoder.py:260: UserWarning: During the encoding, NaN values were introduced in the feature(s) pegi_fr.\n",
      "  warnings.warn(\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "80 fits failed out of a total of 160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/sklearn/pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/catboost/core.py\", line 5807, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
      "  File \"/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/catboost/core.py\", line 2381, in _fit\n",
      "    train_params = self._prepare_train_params(\n",
      "  File \"/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/catboost/core.py\", line 2307, in _prepare_train_params\n",
      "    _check_train_params(params)\n",
      "  File \"_catboost.pyx\", line 6382, in _catboost._check_train_params\n",
      "  File \"_catboost.pyx\", line 6404, in _catboost._check_train_params\n",
      "_catboost.CatBoostError: /Users/zomb-ml-platform-msk/go-agent-21.2.0/pipelines/BuildMaster/catboost.git/catboost/private/libs/options/json_helper.h:41: Can't parse parameter \"depth\" with value: 6.5\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/cyriljulliard/simplon/Movie_Popularity_Prediction/env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [            nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      "             nan             nan             nan             nan\n",
      " -4.70805601e+10 -4.68902508e+10 -4.70125935e+10 -4.67887046e+10\n",
      " -4.60007596e+10 -4.67387686e+10 -4.59903427e+10 -4.66950894e+10\n",
      " -4.66130310e+10 -4.68101346e+10 -4.66118915e+10 -4.68071520e+10\n",
      " -4.63999258e+10 -4.66837806e+10 -4.64054807e+10 -4.66506035e+10]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs hyperparamètres trouvés:\n",
      "{'catboostregressor__depth': 7, 'catboostregressor__l2_leaf_reg': 0.5, 'catboostregressor__learning_rate': 0.055, 'catboostregressor__n_estimators': 154, 'catboostregressor__random_strength': 0.3}\n",
      "Meilleur score sur les données d'entraînement: -45990342676.86013\n",
      "Score : 0.7373722692738811\n",
      "Score R2 : 0.7373722692738811\n",
      "Score RMSE 222288.53345123585\n",
      "Score MAE : 112614.09939791913\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from category_encoders import CountFrequencyEncoder\n",
    "\n",
    "# Vos données X et y\n",
    "X = data.drop(\"entrees_fr\", axis=1)\n",
    "y = data.entrees_fr\n",
    "cols_keep = [\"budget\", \"duree\", \"franchise\", \"genres\", \"pegi_fr\", \"pegi_usa\", \"entrees_usa\", \"salles_fr\", \"studio\", \"is_compositeur\", \"annee\", \"origine\"]\n",
    "# cols_keep = [\"budget\", \"duree\", \"franchise\", \"genres\", \"pegi_fr\",  \"salles_fr\", \"annee\", \"origine\"]\n",
    "X = X[cols_keep]\n",
    "\n",
    "X[\"franchise\"] = X[\"franchise\"].astype(str)\n",
    "X[\"genres\"] = X[\"genres\"].astype(str)\n",
    "X[\"pegi_fr\"] = X[\"pegi_fr\"].astype(str)\n",
    "X[\"origine\"] = X[\"origine\"].astype(str)\n",
    "\n",
    "X[\"pegi_usa\"] = X[\"pegi_usa\"].astype(str)\n",
    "X[\"is_compositeur\"] = X[\"is_compositeur\"].astype(str)\n",
    "X[\"annee\"] = X[\"annee\"].astype(str)\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définition des colonnes catégorielles, numériques et année\n",
    "object_cols = list(X.select_dtypes(include=[\"object\"]).columns)\n",
    "cat_cols = list(X.select_dtypes(include=[\"category\"]).columns)#.drop([\"annee\"]))\n",
    "num_cols = list(X.select_dtypes(include=[\"int64\"]).columns.drop([\"annee\"]))\n",
    "year_col = [\"annee\"]\n",
    "\n",
    "# Définition du prétraitement des données\n",
    "unique_years = sorted(data[\"annee\"].unique())\n",
    "preprocessing = ColumnTransformer([\n",
    "        # (\"onehot\", OneHotEncoder(), cat_cols),\n",
    "        (\"frequency\", CountFrequencyEncoder(encoding_method=\"frequency\", missing_values=\"ignore\"), object_cols),\n",
    "        (\"scaler\", StandardScaler(), num_cols),\n",
    "        (\"ordinal\", OrdinalEncoder(categories=[unique_years], handle_unknown=\"use_encoded_value\", unknown_value=2000), year_col),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "pre_fit = preprocessing.fit(X_train)\n",
    "fit_cols = pre_fit.get_feature_names_out()\n",
    "\n",
    "cat_indices = []\n",
    "for i, col_name in enumerate(fit_cols):\n",
    "    if col_name in cat_cols:\n",
    "        cat_indices.append(i)\n",
    "\n",
    "# Création du pipeline\n",
    "pipe_cb = make_pipeline(preprocessing, CatBoostRegressor(one_hot_max_size=70, verbose=0, cat_features=cat_indices, random_state=42))\n",
    "\n",
    "# Définition de la grille d'hyperparamètres à rechercher\n",
    "# {'catboostregressor__depth': 7, 'catboostregressor__l2_leaf_reg': 0.6, 'catboostregressor__learning_rate': 0.055, \n",
    "#  'catboostregressor__n_estimators': 154, 'catboostregressor__random_strength': 0.31}\n",
    "\n",
    "param_grid = {\n",
    "    \"catboostregressor__learning_rate\": [0.05, 0.055],\n",
    "    \"catboostregressor__depth\": [6.5, 7],\n",
    "    \"catboostregressor__l2_leaf_reg\": [ 0.5, 0.6],\n",
    "    \"catboostregressor__n_estimators\": [ 149, 154],\n",
    "    \"catboostregressor__random_strength\": [ 0.30, 0.31],\n",
    "}\n",
    "\n",
    "# Recherche des meilleurs hyperparamètres avec validation croisée\n",
    "grid_search = GridSearchCV(pipe_cb, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Affichage des meilleurs hyperparamètres et de la performance\n",
    "print(\"Meilleurs hyperparamètres trouvés:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Meilleur score sur les données d'entraînement:\", grid_search.best_score_)\n",
    "\n",
    "# Prédiction sur les données de test avec les meilleurs paramètres\n",
    "pred_test = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calcul des métriques de performance\n",
    "score = grid_search.best_estimator_.score(X_test, y_test)\n",
    "r2 = r2_score(y_test, pred_test)\n",
    "mse = mean_squared_error(y_test, pred_test)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, pred_test)\n",
    "\n",
    "print(\"Score :\", score)\n",
    "print(\"Score R2 :\", r2)\n",
    "print(\"Score RMSE\", rmse)\n",
    "print(\"Score MAE :\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>salles_fr</td>\n",
       "      <td>49.770122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>annee</td>\n",
       "      <td>14.683647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>budget</td>\n",
       "      <td>12.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>duree</td>\n",
       "      <td>7.391976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>genres</td>\n",
       "      <td>6.846947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>franchise</td>\n",
       "      <td>6.543437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>origine</td>\n",
       "      <td>1.824327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pegi_fr</td>\n",
       "      <td>0.619545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    features      score\n",
       "6  salles_fr  49.770122\n",
       "7      annee  14.683647\n",
       "4     budget  12.320000\n",
       "5      duree   7.391976\n",
       "1     genres   6.846947\n",
       "0  franchise   6.543437\n",
       "3    origine   1.824327\n",
       "2    pegi_fr   0.619545"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe_cb.fit(X_train, y_train)\n",
    "preprocessed_features = pipe_cb.named_steps['columntransformer'].get_feature_names_out()\n",
    "cb_model = pipe_cb.named_steps['catboostregressor']\n",
    "\n",
    "feat_imp = pd.DataFrame({'features': preprocessed_features, 'score': cb_model.feature_importances_})\n",
    "feat_imp.sort_values(by=['score'], ascending=False, inplace=True)\n",
    "display(feat_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline XGBoost par pays: Etats-Unis, France, other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs hyperparamètres pour les films originaires des États-Unis:\n",
      "{'xgbregressor__colsample_bytree': 0.9, 'xgbregressor__gamma': 0, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 4, 'xgbregressor__min_child_weight': 0.5, 'xgbregressor__n_estimators': 100, 'xgbregressor__reg_alpha': 0.05, 'xgbregressor__reg_lambda': 0.01, 'xgbregressor__scale_pos_weight': 0, 'xgbregressor__subsample': 1.0}\n",
      "Meilleurs hyperparamètres pour les films originaires de France:\n",
      "{'xgbregressor__colsample_bytree': 0.9, 'xgbregressor__gamma': 0, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 4, 'xgbregressor__min_child_weight': 0.5, 'xgbregressor__n_estimators': 100, 'xgbregressor__reg_alpha': 0.05, 'xgbregressor__reg_lambda': 0.01, 'xgbregressor__scale_pos_weight': 0, 'xgbregressor__subsample': 1.0}\n",
      "Meilleurs hyperparamètres pour les films originaires d'autres pays:\n",
      "{'xgbregressor__colsample_bytree': 0.9, 'xgbregressor__gamma': 0, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 4, 'xgbregressor__min_child_weight': 0.5, 'xgbregressor__n_estimators': 100, 'xgbregressor__reg_alpha': 0.05, 'xgbregressor__reg_lambda': 0.01, 'xgbregressor__scale_pos_weight': 0, 'xgbregressor__subsample': 1.0}\n",
      "Score pour les films originaires des États-Unis : 0.8109277736283157\n",
      "Score RMSE pour les films originaires des États-Unis : 202762.55090467367\n",
      "Score pour les films originaires de France : 0.619865699758454\n",
      "Score RMSE pour les films originaires de France : 247478.02221978735\n",
      "Score pour les films originaires d'autres pays : 0.8527950556934449\n",
      "Score RMSE pour les films originaires d'autres pays : 72775.54417691109\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from category_encoders.count import CountEncoder\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "X = data.drop(\"entrees_fr\", axis=1)\n",
    "y = data.entrees_fr\n",
    "\n",
    "# cols_drop = [\"acteurs\", \"compositeur\", \"date\", \"pays\", \"producteur\", \"realisateur\", \"titre\"]\n",
    "# Liste des colonnes à garder\n",
    "cols_keep = [\"budget\", \"duree\", \"franchise\", \"genres\", \"pegi_fr\", \"pegi_usa\", \"entrees_usa\", \"salles_fr\", \"studio\", \"is_compositeur\", \"annee\", \"origine\"]\n",
    "X = X[cols_keep]\n",
    "\n",
    "X[\"franchise\"] = X[\"franchise\"].astype(str)\n",
    "X[\"genres\"] = X[\"genres\"].astype(str)\n",
    "X[\"pegi_fr\"] = X[\"pegi_fr\"].astype(str)\n",
    "X[\"pegi_usa\"] = X[\"pegi_usa\"].astype(str)\n",
    "X[\"is_compositeur\"] = X[\"is_compositeur\"].astype(str)\n",
    "# X[\"annee\"] = X[\"annee\"].astype(str)\n",
    "X[\"origine\"] = X[\"origine\"].astype(str)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=42)\n",
    "\n",
    "object_cols = list(X.select_dtypes(include=[\"object\"]).columns)\n",
    "cat_cols = list(X.select_dtypes(include=[\"category\"]).columns)\n",
    "num_cols = list(X.select_dtypes(include=[\"int64\"]).columns.drop([\"annee\"]))\n",
    "year_col = [\"annee\"]\n",
    "\n",
    "# Ordinal encoding for ApprovalFY\n",
    "unique_years = sorted(data[\"annee\"].unique())\n",
    "preprocessing = ColumnTransformer([\n",
    "        (\"onehot\", OneHotEncoder(), cat_cols),\n",
    "        (\"frequency\", CountEncoder(), object_cols),\n",
    "        (\"scaler\", StandardScaler(), num_cols),\n",
    "        (\"ordinal\", OrdinalEncoder(categories=[unique_years], handle_unknown=\"use_encoded_value\", unknown_value=2000), year_col),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Définition de la grille des hyperparamètres à tester\n",
    "# {'xgbregressor__colsample_bytree': 0.9, 'xgbregressor__gamma': 0, 'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 4, \n",
    "#'xgbregressor__min_child_weight': 0.5, 'xgbregressor__n_estimators': 100, 'xgbregressor__reg_alpha': 0.05, 'xgbregressor__reg_lambda': 0.01,\n",
    "# 'xgbregressor__scale_pos_weight': 1, 'xgbregressor__subsample': 1.0}\n",
    "\n",
    "param_grid = {\n",
    "    'xgbregressor__n_estimators': [100],#[50, 100, 150],\n",
    "    'xgbregressor__learning_rate': [0.1],#[0.5, 0.1, 1.5],\n",
    "    'xgbregressor__max_depth': [4],#[3.4, 4, 4.5],\n",
    "    'xgbregressor__min_child_weight': [0.5],#[0.5, 1, 2],\n",
    "    'xgbregressor__gamma': [0],#[0.0, 0.01, 0.02],\n",
    "    'xgbregressor__subsample': [1.0],#[0.8, 0.9, 1.0],\n",
    "    'xgbregressor__colsample_bytree': [0.9],#[0.8, 0.9, 1.0],\n",
    "    'xgbregressor__reg_alpha': [0.05],#[0, 0.001, 0.005, 0.01, 0.05],\n",
    "    'xgbregressor__reg_lambda': [0.01],#[0, 0.001, 0.005, 0.01, 0.05],\n",
    "    'xgbregressor__scale_pos_weight': [0],#[1, 2, 3, 4, 5],\n",
    "\n",
    "    # Ajoutez d'autres hyperparamètres à tester selon vos besoins\n",
    "}\n",
    "\n",
    "# Création du pipeline\n",
    "pipe_xgb = make_pipeline(preprocessing, xgb_model)\n",
    "\n",
    "# Séparation des données par origine\n",
    "X_usa = X_train[X_train['origine'] == 'etats-unis']\n",
    "y_usa = y_train[X_train['origine'] == 'etats-unis']\n",
    "X_france = X_train[X_train['origine'] == 'france']\n",
    "y_france = y_train[X_train['origine'] == 'france']\n",
    "X_other = X_train[(X_train['origine'] != 'etats-unis') & (X_train['origine'] != 'france')]\n",
    "y_other = y_train[(X_train['origine'] != 'etats-unis') & (X_train['origine'] != 'france')]\n",
    "\n",
    "# Entraînement du modèle pour les films originaires des États-Unis\n",
    "grid_search_usa = GridSearchCV(pipe_xgb, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_usa.fit(X_usa, y_usa)\n",
    "print(\"Meilleurs hyperparamètres pour les films originaires des États-Unis:\")\n",
    "print(grid_search_usa.best_params_)\n",
    "\n",
    "# Entraînement du modèle pour les films originaires de France\n",
    "grid_search_france = GridSearchCV(pipe_xgb, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_france.fit(X_france, y_france)\n",
    "print(\"Meilleurs hyperparamètres pour les films originaires de France:\")\n",
    "print(grid_search_france.best_params_)\n",
    "\n",
    "# Entraînement du modèle pour les films originaires d'autres pays\n",
    "grid_search_other = GridSearchCV(pipe_xgb, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_other.fit(X_other, y_other)\n",
    "print(\"Meilleurs hyperparamètres pour les films originaires d'autres pays:\")\n",
    "print(grid_search_other.best_params_)\n",
    "\n",
    "# Prédictions pour les films originaires des États-Unis\n",
    "pred_usa = grid_search_usa.predict(X_test[X_test['origine'] == 'etats-unis'])\n",
    "y_test_usa = y_test[X_test['origine'] == 'etats-unis']\n",
    "\n",
    "# Prédictions pour les films originaires de France\n",
    "pred_france = grid_search_france.predict(X_test[X_test['origine'] == 'france'])\n",
    "y_test_france = y_test[X_test['origine'] == 'france']\n",
    "\n",
    "# Prédictions pour les films originaires d'autres pays\n",
    "pred_other = grid_search_other.predict(X_test[(X_test['origine'] != 'etats-unis') & (X_test['origine'] != 'france')])\n",
    "y_test_other = y_test[(X_test['origine'] != 'etats-unis') & (X_test['origine'] != 'france')]\n",
    "\n",
    "# Calcul des métriques pour chaque groupe\n",
    "mse_usa = mean_squared_error(y_test_usa, pred_usa)\n",
    "r2_usa = r2_score(y_test_usa, pred_usa)\n",
    "print(\"Score pour les films originaires des États-Unis :\", r2_usa)\n",
    "print(\"Score RMSE pour les films originaires des États-Unis :\", np.sqrt(mse_usa))\n",
    "\n",
    "mse_france = mean_squared_error(y_test_france, pred_france)\n",
    "r2_fr = r2_score(y_test_france, pred_france)\n",
    "print(\"Score pour les films originaires de France :\", r2_fr)\n",
    "print(\"Score RMSE pour les films originaires de France :\", np.sqrt(mse_france))\n",
    "\n",
    "mse_other = mean_squared_error(y_test_other, pred_other)\n",
    "r2_other = r2_score(y_test_other, pred_other)\n",
    "print(\"Score pour les films originaires d'autres pays :\", r2_other)\n",
    "print(\"Score RMSE pour les films originaires d'autres pays :\", np.sqrt(mse_other))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export du modele Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/model_cb.pkl\", \"wb\") as f:\n",
    "  pickle.dump(pipe_cb, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
